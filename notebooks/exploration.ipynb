{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aspect-Based Sentiment Analysis Exploration\n",
        "\n",
        "This notebook demonstrates the exploration and testing of different ABSA implementations.\n",
        "\n",
        "## Overview\n",
        "- **Lexicon-based ABSA**: Using spaCy and sentiment lexicons\n",
        "- **Transformer-based ABSA**: Using pre-trained Hugging Face models\n",
        "- **LLM-based ABSA**: Using local LLM through Ollama\n",
        "\n",
        "## Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
        "\n",
        "from base import AspectSentiment\n",
        "from lexicon_absa import LexiconABSA\n",
        "from transformer_absa import TransformerABSA\n",
        "from llm_absa import LLMABSA\n",
        "from utils import (\n",
        "    load_test_data, save_results, calculate_accuracy, \n",
        "    calculate_precision_recall_f1, benchmark_analyzer,\n",
        "    create_sample_data, print_analysis_results, ensure_data_directory\n",
        ")\n",
        "\n",
        "print(\"Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Analyzers\n",
        "\n",
        "Let's initialize all three ABSA implementations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analyzers = {}\n",
        "results = {}\n",
        "\n",
        "print(\"Initializing analyzers...\")\n",
        "\n",
        "try:\n",
        "    lexicon_analyzer = LexiconABSA()\n",
        "    analyzers['Lexicon'] = lexicon_analyzer\n",
        "    print(\"✓ LexiconABSA initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ LexiconABSA failed to initialize: {e}\")\n",
        "\n",
        "try:\n",
        "    transformer_analyzer = TransformerABSA()\n",
        "    analyzers['Transformer'] = transformer_analyzer\n",
        "    print(\"✓ TransformerABSA initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ TransformerABSA failed to initialize: {e}\")\n",
        "\n",
        "try:\n",
        "    llm_analyzer = LLMABSA()\n",
        "    analyzers['LLM'] = llm_analyzer\n",
        "    print(\"✓ LLMABSA initialized successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ LLMABSA failed to initialize: {e}\")\n",
        "\n",
        "print(f\"\\nSuccessfully initialized {len(analyzers)} analyzers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Test Data\n",
        "\n",
        "Let's load our test data and create sample data if needed:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_file = ensure_data_directory()\n",
        "test_data = load_test_data(data_file)\n",
        "\n",
        "if not test_data:\n",
        "    print(\"Creating sample test data...\")\n",
        "    test_data = create_sample_data()\n",
        "    save_results(test_data, data_file)\n",
        "\n",
        "print(f\"Loaded {len(test_data)} test samples\")\n",
        "print(\"\\nSample test data:\")\n",
        "for i, item in enumerate(test_data[:3]):\n",
        "    print(f\"{i+1}. {item['text']}\")\n",
        "    print(f\"   Expected: {[f\\\"{a['aspect']} ({a['sentiment']})\\\" for a in item['ground_truth']]}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Individual Testing\n",
        "\n",
        "Let's test each analyzer individually on sample texts:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_texts = [\n",
        "    \"The pizza was delicious but the service was terrible.\",\n",
        "    \"The laptop has great performance and excellent battery life.\",\n",
        "    \"The hotel room was clean and comfortable, but the WiFi was slow.\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for name, analyzer in analyzers.items():\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            results = analyzer.analyze(text)\n",
        "            processing_time = time.time() - start_time\n",
        "            \n",
        "            print(f\"\\n{name} Results ({processing_time:.3f}s):\")\n",
        "            if results:\n",
        "                for result in results:\n",
        "                    print(f\"  - {result.aspect}: {result.sentiment} (confidence: {result.confidence:.3f})\")\n",
        "            else:\n",
        "                print(\"  No aspects found\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"\\n{name} Error: {e}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
